from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import re
import json
import time
from bs4 import BeautifulSoup
import os

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# –°–ø–∏—Å–æ–∫ URL –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
FILTER_URLS = [
    "https://kolesa.kz/cars/?sort_by=add_date-desc",
    "https://kolesa.kz/cars/?sort_by=price-asc",
    "https://kolesa.kz/cars/?sort_by=year.price-desc.asc",
    "https://kolesa.kz/cars/?sort_by=price-desc",
]

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–Ω–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
data_dir = os.path.abspath(os.path.join(os.getcwd(), "data/unprocessed_json"))
os.makedirs(data_dir, exist_ok=True)

# –§–∞–π–ª –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
last_page_file = os.path.join(data_dir, "last_page.json")

def load_last_page():
    if os.path.exists(last_page_file):
        with open(last_page_file, "r") as f:
            return json.load(f)
    return {}

def save_last_page(data):
    with open(last_page_file, "w") as f:
        json.dump(data, f, indent=4)

def get_car_links(url, pages=15, start_page=1):
    links = set()
    pattern = re.compile(r"https://kolesa.kz/a/show/\d+")

    for page in range(start_page, start_page + pages):
        if len(links) >= 100:
            break
        full_url = f"{url}?page={page}"
        try:
            print(f"üîç –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {full_url}")
            response = requests.get(full_url, headers=HEADERS, timeout=10)
            if response.status_code != 200:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {full_url} - —Å—Ç–∞—Ç—É—Å {response.status_code}")
                time.sleep(60)
                continue

            soup = BeautifulSoup(response.text, "html.parser")
            found_links = {"https://kolesa.kz" + a["href"] for a in soup.select('a[href^="/a/show/"]')}
            filtered_links = {link for link in found_links if pattern.match(link)}

            links.update(filtered_links)
            print(f"üîó –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(filtered_links)} —Å—Å—ã–ª–æ–∫")

            time.sleep(30)

        except requests.exceptions.RequestException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {full_url}: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 60 —Å–µ–∫—É–Ω–¥.")
            time.sleep(60)

    print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
    return list(links)[:100]


def get_car_data(url, processed_ids):
    retries = 3
    for attempt in range(retries):
        try:
            print(f"üåê –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–µ: {url}")
            response = requests.get(url, headers=HEADERS, timeout=10)
            if response.status_code == 200:
                html = response.text
                match = re.search(r"window\.digitalData\s*=\s*({.*?});", html, re.DOTALL)
                if match:
                    data = json.loads(match.group(1))
                    car = data.get("product", {})
                    car_id = car.get("id")
                    if car_id and car_id not in processed_ids:
                        processed_ids.add(car_id)
                        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã –¥–ª—è: {url}")

                        # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–Ω—É—Ç—Ä–∏ div —Å –∫–ª–∞—Å—Å–æ–º offer__parameters
                        soup = BeautifulSoup(html, 'html.parser')
                        offer_params_div = soup.find('div', class_='offer__parameters')

                        params_data = {}
                        if offer_params_div:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ <dl> —ç–ª–µ–º–µ–Ω—Ç—ã —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                            param_dls = offer_params_div.find_all('dl')
                            for param_dl in param_dls:
                                param_name_tag = param_dl.find('dt')
                                param_value_tag = param_dl.find('dd')

                                if param_name_tag and param_value_tag:
                                    param_name = param_name_tag.get_text(strip=True)
                                    param_value = param_value_tag.get_text(strip=True)
                                    params_data[param_name] = param_value

                        # –ï—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–π–¥–µ–Ω—ã, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –∫ –¥–∞–Ω–Ω—ã–º –∞–≤—Ç–æ–º–æ–±–∏–ª—è
                        if params_data:
                            car['parameters'] = params_data
                        return car
                    else:
                        print(f"‚ö†Ô∏è –ü–æ–≤—Ç–æ—Ä –∑–∞–ø–∏—Å–∏ —Å ID {car_id}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                else:
                    print(f"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è: {url}")
                return None
            elif response.status_code in [403, 429, 503]:
                print(f"üö® –°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –ø–∞—Ä—Å–∏–Ω–≥ (–∫–æ–¥ {response.status_code}). –û–∂–∏–¥–∞–µ–º 300 —Å–µ–∫—É–Ω–¥.")
                time.sleep(300)
            else:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {url}, —Å—Ç–∞—Ç—É—Å {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 60 —Å–µ–∫—É–Ω–¥.")
            time.sleep(60)

    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")
    return None


def parse_cars(url, label="filtered", pages=50, max_links=20):
    print(f"üöó –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {url} | –ú–µ—Ç–∫–∞: {label} | –°—Ç—Ä–∞–Ω–∏—Ü: {pages}")
    last_pages = load_last_page()
    start_page = last_pages.get(label, 1)
    cars = []
    processed_ids = set()
    links = get_car_links(url, pages, start_page)[:max_links]
    for i, link in enumerate(links):
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Å—ã–ª–∫–∏ {i+1}/{len(links)}: {link}")
        car = get_car_data(link, processed_ids)
        if car:
            cars.append(car)
        time.sleep(2)

    if cars:
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        file_path = os.path.join(data_dir, f"cars_{label}_{timestamp}.json")
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(cars, f, indent=4, ensure_ascii=False)
        print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(cars)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π! –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")

        last_pages[label] = start_page + pages
        save_last_page(last_pages)
    else:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")

def parse_new_cars():
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
    parse_cars("https://kolesa.kz/cars/", label="new", pages=20, max_links=1000)

def parse_filtered_cars():
    for url in FILTER_URLS:
        label = url.split("=")[-1]
        print(f"üõ†Ô∏è –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º: {label}")
        parse_cars(url, label=label, pages=5, max_links=100)

default_args = {
    "owner": "temirlan",
    "depends_on_past": False,
    "start_date": datetime(2024, 2, 14),
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "kolesa_car_parser",
    default_args=default_args,
    schedule_interval="0 */1 * * *",  # –ö–∞–∂–¥—ã–µ 4 —á–∞—Å–∞
    catchup=False,
    tags=["kolesa", "parser"]
)

new_cars_task = PythonOperator(
    task_id="parse_new_cars",
    python_callable=parse_new_cars,
    dag=dag
)

filtered_cars_task = PythonOperator(
    task_id="parse_filtered_cars",
    python_callable=parse_filtered_cars,
    dag=dag,
)

new_cars_task >> filtered_cars_task

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import re
import json
import time
from bs4 import BeautifulSoup
import os

# Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº URL Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
FILTER_URLS = [
    "https://kolesa.kz/cars/?sort_by=add_date-desc",
    "https://kolesa.kz/cars/?sort_by=price-asc",
    "https://kolesa.kz/cars/?sort_by=year.price-desc.asc",
    "https://kolesa.kz/cars/?sort_by=price-desc",
]

# Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²Ğ½Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ°
data_dir = os.path.abspath(os.path.join(os.getcwd(), "data/unprocessed_json"))
os.makedirs(data_dir, exist_ok=True)

# Ğ¤Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
last_page_file = os.path.join(data_dir, "last_page.json")

def load_last_page():
    if os.path.exists(last_page_file):
        with open(last_page_file, "r") as f:
            return json.load(f)
    return {}

def save_last_page(data):
    with open(last_page_file, "w") as f:
        json.dump(data, f, indent=4)

def get_car_links(url, pages=15, start_page=1):
    links = set()
    pattern = re.compile(r"https://kolesa.kz/a/show/\d+")

    for page in range(start_page, start_page + pages):
        if len(links) >= 100:
            break
        full_url = f"{url}?page={page}"
        try:
            print(f"ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹: {full_url}")
            response = requests.get(full_url, headers=HEADERS, timeout=10)
            if response.status_code != 200:
                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: {full_url} - ÑÑ‚Ğ°Ñ‚ÑƒÑ {response.status_code}")
                time.sleep(60)
                continue

            soup = BeautifulSoup(response.text, "html.parser")
            found_links = {"https://kolesa.kz" + a["href"] for a in soup.select('a[href^="/a/show/"]')}
            filtered_links = {link for link in found_links if pattern.match(link)}

            links.update(filtered_links)
            print(f"ğŸ”— Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {page}: Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(filtered_links)} ÑÑÑ‹Ğ»Ğ¾Ğº")

            time.sleep(30)

        except requests.exceptions.RequestException as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ {full_url}: {e}. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· 60 ÑĞµĞºÑƒĞ½Ğ´.")
            time.sleep(60)

    print(f"âœ… Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ ÑÑÑ‹Ğ»Ğ¾Ğº: {len(links)}")
    return list(links)[:100]

def get_car_data(url, processed_ids):
    retries = 3
    for attempt in range(retries):
        try:
            print(f"ğŸŒ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞµ: {url}")
            response = requests.get(url, headers=HEADERS, timeout=10)
            if response.status_code == 200:
                html = response.text
                match = re.search(r"window\.digitalData\s*=\s*({.*?});", html, re.DOTALL)
                if match:
                    data = json.loads(match.group(1))
                    car = data.get("product", {})
                    car_id = car.get("id")
                    if car_id and car_id not in processed_ids:
                        processed_ids.add(car_id)
                        print(f"âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ñ‹ Ğ´Ğ»Ñ: {url}")
                        return car
                    else:
                        print(f"âš ï¸ ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ ID {car_id}, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼")
                else:
                    print(f"âš ï¸ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ´Ğ»Ñ: {url}")
                return None
            elif response.status_code in [403, 429, 503]:
                print(f"ğŸš¨ Ğ¡Ğ°Ğ¹Ñ‚ Ğ·Ğ°Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ (ĞºĞ¾Ğ´ {response.status_code}). ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ 300 ÑĞµĞºÑƒĞ½Ğ´.")
                time.sleep(300)
            else:
                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ: {url}, ÑÑ‚Ğ°Ñ‚ÑƒÑ {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ {url}: {e}. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· 60 ÑĞµĞºÑƒĞ½Ğ´.")
            time.sleep(60)

    print(f"âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»Ğµ {retries} Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº: {url}")
    return None

def parse_cars(url, label="filtered", pages=50, max_links=20):
    print(f"ğŸš— ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³: {url} | ĞœĞµÑ‚ĞºĞ°: {label} | Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {pages}")
    last_pages = load_last_page()
    start_page = last_pages.get(label, 1)
    cars = []
    processed_ids = set()
    links = get_car_links(url, pages, start_page)[:max_links]
    for i, link in enumerate(links):
        print(f"ğŸ”„ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑÑÑ‹Ğ»ĞºĞ¸ {i+1}/{len(links)}: {link}")
        car = get_car_data(link, processed_ids)
        if car:
            cars.append(car)
        time.sleep(5)

    if cars:
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        file_path = os.path.join(data_dir, f"cars_{label}_{timestamp}.json")
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(cars, f, indent=4, ensure_ascii=False)
        print(f"âœ… Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ {len(cars)} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹! Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² {file_path}")

        last_pages[label] = start_page + pages
        save_last_page(last_pages)
    else:
        print("âš ï¸ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ.")

def parse_new_cars():
    print("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹")
    parse_cars("https://kolesa.kz/cars/", label="new", pages=100, max_links=100)

def parse_filtered_cars():
    for url in FILTER_URLS:
        label = url.split("=")[-1]
        print(f"ğŸ› ï¸ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ¼: {label}")
        parse_cars(url, label=label, pages=20, max_links=20)

default_args = {
    "owner": "temirlan",
    "depends_on_past": False,
    "start_date": datetime(2024, 2, 14),
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "kolesa_car_parser",
    default_args=default_args,
    schedule_interval="0 */4 * * *",  # ĞšĞ°Ğ¶Ğ´Ñ‹Ğµ 4 Ñ‡Ğ°ÑĞ°
    catchup=False,
    tags=["kolesa", "parser"]
)

new_cars_task = PythonOperator(
    task_id="parse_new_cars",
    python_callable=parse_new_cars,
    dag=dag
)

filtered_cars_task = PythonOperator(
    task_id="parse_filtered_cars",
    python_callable=parse_filtered_cars,
    dag=dag,
)

new_cars_task >> filtered_cars_task
